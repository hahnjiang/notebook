{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def routing(input, b):\n",
    "    W = tf.get_variable('W', shape=(1, 1152, 10, 8, 16), dtype=tf.float32)\n",
    "    input = tf.tile(input, [1, 1, 10, 1, 1])\n",
    "    W = tf.tile(W, [batch_size, 1, 1, 1, 1])\n",
    "    u_hat = tf.matmul(W, input, transpose_a=True)\n",
    "\n",
    "    for iter in range(iterations):\n",
    "        c = tf.nn.softmax(b, dim=2)\n",
    "        c = tf.tile(c, [batch_size, 1, 1, 1, 1])\n",
    "        s = tf.multiply(c, u_hat)\n",
    "        s = tf.reduce_sum(s, axis=1, keep_dims=True)\n",
    "        v = squash(s)\n",
    "        v_tiled = tf.tile(v, [1, 1152, 1, 1, 1])\n",
    "        u_produce_v = tf.matmul(u_hat, v_tiled, transpose_a=True)\n",
    "        b += tf.reduce_sum(u_produce_v, axis=0, keep_dims=True)\n",
    "\n",
    "    return(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('conv1'):\n",
    "    conv1 = tf.contrib.layers.conv2d(X, num_outputs=256, kernel_size=9, stride=1, padding='VALID')\n",
    "with tf.variable_scope('primary_capsule'):\n",
    "    primaryCaps = CapsLayer(num_outputs=32, vec_len=8, with_routing=False, layer_type='CONV')\n",
    "    caps1 = primaryCaps(conv1, kernel_size=9, stride=2)\n",
    "with tf.variable_scope('digit_capsule'):\n",
    "    digitCaps = CapsLayer(num_outputs=10, vec_len=16, with_routing=True, layer_type='FC')\n",
    "    caps2 = digitCaps(caps1)\n",
    "with tf.variable_scope('masking'):\n",
    "    v_length = tf.sqrt(tf.reduce_sum(tf.square(caps2), axis=2, keep_dims=True) + epsilon)\n",
    "    softmax_v = tf.nn.softmax(v_length, dim=1)\n",
    "    argmax_idx = tf.to_int32(tf.argmax(softmax_v, axis=1))\n",
    "    argmax_idx = tf.reshape(argmax_idx, shape=(batch_size, ))\n",
    "    if not mask_with_y:\n",
    "        masked_v = []\n",
    "        for batch_size in range(batch_size):\n",
    "            v = caps2[batch_size][argmax_idx[batch_size], :]\n",
    "            masked_v.append(tf.reshape(v, shape=(1, 1, 16, 1)))\n",
    "            masked_v = tf.concat(masked_v, axis=0)\n",
    "    else:\n",
    "        masked_v = tf.multiply(tf.squeeze(caps2), tf.reshape(Y, (-1, 10, 1)))\n",
    "        v_length = tf.sqrt(tf.reduce_sum(tf.square(caps2), axis=2, keep_dims=True) + epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector_j = tf.reshape(masked_v, shape=(batch_size, -1))\n",
    "fc1 = tf.contrib.layers.fully_connected(vector_j, num_outputs=512)\n",
    "fc2 = tf.contrib.layers.fully_connected(fc1, num_outputs=1024)\n",
    "decoded = tf.contrib.layers.fully_connected(fc2, num_outputs=784, activation_fn=tf.sigmoid)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
